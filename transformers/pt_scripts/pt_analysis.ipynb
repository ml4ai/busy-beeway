{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f75156-51c6-4484-a926-5e76c8c968a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import orbax.checkpoint as ocp\n",
    "import seaborn as sns\n",
    "from flax import nnx\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"../..\"))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "from transformers.data_utils.bb_data_loading import load_list\n",
    "from transformers.models.pref_transformer import load_PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838a9fb-8439-483c-bcc9-d320d9e69569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_list = load_list(\"../participant_list.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e494b8f-fda5-4829-af8a-870c4321b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_id = np.random.choice(p_list)\n",
    "with h5py.File(f\"../participant_trajectories/state_data_1/{p_id}.hdf5\") as f:\n",
    "    r_idx = np.random.choice(f[\"states\"].shape[0])\n",
    "    sts = f[\"states\"][r_idx, ...].reshape(1, 100, 16)\n",
    "    acts = f[\"actions\"][r_idx, ...].reshape(1, 100, 3)\n",
    "    ts = f[\"timesteps\"][r_idx, ...].reshape(1, 100)\n",
    "    am = f[\"attn_mask\"][r_idx, ...].reshape(1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e58ac4-b87f-4178-8aa2-4346def8c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rewards = []\n",
    "n_all_weights = []\n",
    "n_max_reward = -np.inf\n",
    "n_min_reward = np.inf\n",
    "\n",
    "n_max_weight = -np.inf\n",
    "n_min_weight = np.inf\n",
    "\n",
    "all_rewards = []\n",
    "\n",
    "pr = []\n",
    "\n",
    "reward_function = os.path.expanduser(\n",
    "    \"~/busy-beeway/transformers/t0012/pt_rewards/best_model.ckpt\"\n",
    ")\n",
    "checkpointer = ocp.Checkpointer(ocp.CompositeCheckpointHandler())\n",
    "r_model = load_PT(reward_function, checkpointer, on_cpu=True)\n",
    "r_model = nnx.jit(r_model, static_argnums=4)\n",
    "checkpointer.close()\n",
    "for i in range(int(np.max(sts[:, :, 15]))):\n",
    "    sts[:, :, 15] = np.where(am != 0, i, sts[:, :, 15])\n",
    "    n_pred, n_weights = r_model(sts, acts, ts, am, training=False)\n",
    "\n",
    "    n_r_pred = n_pred[\"value\"].reshape(\n",
    "        100,\n",
    "    )\n",
    "\n",
    "    am_sum = int(np.sum(am))\n",
    "    n_r_pred = n_r_pred[:am_sum]\n",
    "    n_max_r_pred = np.max(n_r_pred)\n",
    "    if n_max_r_pred > n_max_reward:\n",
    "        n_max_reward = n_max_r_pred\n",
    "\n",
    "    n_min_r_pred = np.min(n_r_pred)\n",
    "    if n_min_r_pred < n_min_reward:\n",
    "        n_min_reward = n_min_r_pred\n",
    "\n",
    "    n_ts = np.arange(am_sum)\n",
    "    n_rewards.append(np.column_stack([n_ts, n_r_pred]))\n",
    "    all_rewards.append(n_r_pred)\n",
    "    n_weights = n_weights[-1].reshape(1, 100, 100)\n",
    "    n_weights = np.mean(n_weights, axis=1).reshape(\n",
    "        100,\n",
    "    )\n",
    "    n_weights = n_weights[:am_sum]\n",
    "    n_all_weights.append(np.column_stack([n_ts, n_weights]))\n",
    "    n_max_w = np.max(n_weights)\n",
    "    if n_max_w > n_max_weight:\n",
    "        n_max_weight = n_max_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9b8e6-875d-4203-99ac-0eeb94567327",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_control_idxs = []\n",
    "end_control_idxs = []\n",
    "for i in range(100):\n",
    "    if acts[0, i, 2] == 1:\n",
    "        if i == 0:\n",
    "            start_control_idxs.append(i)\n",
    "        else:\n",
    "            if acts[0, i - 1, 2] == 0:\n",
    "                start_control_idxs.append(i)\n",
    "    else:\n",
    "        if acts[0, i - 1, 2] == 1:\n",
    "            end_control_idxs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b697e1a-7708-4d9c-ba7b-c271102c2af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rewards = np.stack(all_rewards)\n",
    "norm_rewards = (all_rewards - np.min(all_rewards, axis=0)) / (\n",
    "    np.max(all_rewards, axis=0) - np.min(all_rewards, axis=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862bae41-812b-4bfa-be1b-5c1be508ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axe = plt.subplots(3, figsize=(15, 10))\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "axe[0].set_xlim(0, am_sum)\n",
    "axe[0].set_ylim(n_min_reward, n_max_reward)\n",
    "n_days = np.arange(len(n_all_weights))\n",
    "n_line_collection0 = LineCollection(n_rewards, array=n_days, cmap=\"rainbow\")\n",
    "axe[0].add_collection(n_line_collection0)\n",
    "axe[0].vlines(\n",
    "    start_control_idxs,\n",
    "    n_min_reward,\n",
    "    n_max_reward,\n",
    "    color=\"green\",\n",
    "    linestyle=\"dashed\",\n",
    "    label=\"Start Human Control\",\n",
    ")\n",
    "axe[0].vlines(\n",
    "    end_control_idxs,\n",
    "    n_min_reward,\n",
    "    n_max_reward,\n",
    "    color=\"magenta\",\n",
    "    linestyle=\"dashed\",\n",
    "    label=\"End Human Control\",\n",
    ")\n",
    "# axe[0,1].errorbar(np.arange(am_sum),np.mean(all_rewards,axis=0),yerr=np.std(all_rewards,axis=0),color=\"black\",label=\"Mean\")\n",
    "# axe[0,1].plot(np.arange(am_sum),np.median(all_rewards,axis=0),color=\"green\",label=\"Median\")\n",
    "axe[0].set_title(\"Reward Signal (t0012)\")\n",
    "axe[0].set_xlabel(\"Timestep\")\n",
    "axe[0].set_ylabel(\"Reward\")\n",
    "axe[0].legend(loc=\"upper left\")\n",
    "fig.colorbar(n_line_collection0, label=\"Day\")\n",
    "\n",
    "axe[1].set_xlim(0, am_sum)\n",
    "axe[1].set_ylim(0, n_max_weight)\n",
    "n_line_collection1 = LineCollection(n_all_weights, array=n_days, cmap=\"rainbow\")\n",
    "axe[1].add_collection(n_line_collection1)\n",
    "axe[1].vlines(\n",
    "    start_control_idxs,\n",
    "    0,\n",
    "    n_max_weight,\n",
    "    color=\"green\",\n",
    "    linestyle=\"dashed\",\n",
    "    label=\"Start Human Control\",\n",
    ")\n",
    "axe[1].vlines(\n",
    "    end_control_idxs,\n",
    "    0,\n",
    "    n_max_weight,\n",
    "    color=\"magenta\",\n",
    "    linestyle=\"dashed\",\n",
    "    label=\"End Human Control\",\n",
    ")\n",
    "axe[1].set_title(\"Importance Weighting (t0012)\")\n",
    "axe[1].set_xlabel(\"Timestep\")\n",
    "axe[1].set_ylabel(\"Weight\")\n",
    "axe[1].legend(loc=\"upper left\")\n",
    "fig.colorbar(n_line_collection1, label=\"Day\")\n",
    "\n",
    "sns.heatmap(norm_rewards, cmap=\"cool\", ax=axe[2], cbar_kws={\"label\": \"Reward\"})\n",
    "axe[2].invert_yaxis()\n",
    "axe[2].set_xlabel(\"Timestep\")\n",
    "axe[2].set_ylabel(\"Day\")\n",
    "axe[2].set_title(\"Relative Reward Signal Heatmap (t0012)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76570a7b-5cd7-40e3-bf78-9812dc236b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(pr)\n",
    "# ax.set_title(\"Preference Prediction (for segment 2) Versus Day\")\n",
    "# ax.set_xlabel(\"Day\")\n",
    "# ax.set_ylabel(\"Probability of Preferred Label\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
