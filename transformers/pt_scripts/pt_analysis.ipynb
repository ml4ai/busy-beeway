{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f75156-51c6-4484-a926-5e76c8c968a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"../..\"))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "from transformers.data_utils.bb_data_loading import load_list\n",
    "from transformers.training.jax_utils import cross_ent_loss\n",
    "from transformers.training.utils import load_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838a9fb-8439-483c-bcc9-d320d9e69569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0012_list = load_list(\"../t0012/preference_data_1/day_list.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e494b8f-fda5-4829-af8a-870c4321b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0012_day = np.random.choice(t0012_list)\n",
    "with h5py.File(f\"../t0012/preference_data_1/{t0012_day}.hdf5\") as f:\n",
    "    r_idx = np.random.choice(f[\"states_2\"].shape[0])\n",
    "    sts_2 = f[\"states_2\"][r_idx, ...].reshape(1, 100, 16)\n",
    "    acts_2 = f[\"actions_2\"][r_idx, ...].reshape(1, 100, 3)\n",
    "    ts_2 = f[\"timesteps_2\"][r_idx, ...].reshape(1, 100)\n",
    "    am_2 = f[\"attn_mask_2\"][r_idx, ...].reshape(1, 100)\n",
    "\n",
    "    sts = f[\"states\"][r_idx, ...].reshape(1, 100, 16)\n",
    "    acts = f[\"actions\"][r_idx, ...].reshape(1, 100, 3)\n",
    "    ts = f[\"timesteps\"][r_idx, ...].reshape(1, 100)\n",
    "    am = f[\"attn_mask\"][r_idx, ...].reshape(1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70f2b78-3198-480f-94ce-034d0fcfb0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with h5py.File(\"../t0012/preference_data_1/test.2022.01.16.09.00.26.hdf5\") as f:\n",
    "#     sts_2 = f[\"states_2\"][6, ...].reshape(1, 100, 16)\n",
    "#     acts_2 = f[\"actions_2\"][6, ...].reshape(1, 100, 3)\n",
    "#     ts_2 = f[\"timesteps_2\"][6, ...].reshape(1, 100)\n",
    "#     am_2 = f[\"attn_mask_2\"][6, ...].reshape(1, 100)\n",
    "\n",
    "#     sts = f[\"states\"][6, ...].reshape(1, 100, 16)\n",
    "#     acts = f[\"actions\"][6, ...].reshape(1, 100, 3)\n",
    "#     ts = f[\"timesteps\"][6, ...].reshape(1, 100)\n",
    "#     am = f[\"attn_mask\"][6, ...].reshape(1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e58ac4-b87f-4178-8aa2-4346def8c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rewards_2 = []\n",
    "n_all_weights_2 = []\n",
    "n_max_reward_2 = -np.inf\n",
    "n_min_reward_2 = np.inf\n",
    "\n",
    "n_max_weight_2 = -np.inf\n",
    "n_min_weight_2 = np.inf\n",
    "\n",
    "all_rewards_2 = []\n",
    "\n",
    "n_rewards = []\n",
    "n_all_weights = []\n",
    "n_max_reward = -np.inf\n",
    "n_min_reward = np.inf\n",
    "\n",
    "n_max_weight = -np.inf\n",
    "n_min_weight = np.inf\n",
    "\n",
    "all_rewards = []\n",
    "\n",
    "pr = []\n",
    "for i in range(len(t0012_list)):\n",
    "    sts_2[:, :, 15] = np.where(am_2 != 0, i, sts_2[:, :, 15])\n",
    "    sts[:, :, 15] = np.where(am != 0, i, sts[:, :, 15])\n",
    "    n_model = load_pickle(\"../t0012/results/pt_rewards/best_model.pkl\")[\"model\"]\n",
    "    n_pred_2, n_weights_2 = n_model._train_state.apply_fn(\n",
    "        n_model._train_state.params, sts_2, acts_2, ts_2, training=False, attn_mask=am_2\n",
    "    )\n",
    "    n_pred, n_weights = n_model._train_state.apply_fn(\n",
    "        n_model._train_state.params, sts, acts, ts, training=False, attn_mask=am\n",
    "    )\n",
    "\n",
    "    n_r_pred_2 = n_pred_2[\"value\"].reshape(\n",
    "        100,\n",
    "    )\n",
    "\n",
    "    n_r_pred = n_pred[\"value\"].reshape(\n",
    "        100,\n",
    "    )\n",
    "\n",
    "    trans_pred_1 = n_pred[\"weighted_sum\"]\n",
    "    trans_pred_2 = n_pred_2[\"weighted_sum\"]\n",
    "\n",
    "    sum_pred_1 = np.nanmean(trans_pred_1.reshape(1, 100), axis=1).reshape(-1, 1)\n",
    "    sum_pred_2 = np.nanmean(trans_pred_2.reshape(1, 100), axis=1).reshape(-1, 1)\n",
    "\n",
    "    pr.append((1.0 / (np.exp(sum_pred_1 - sum_pred_2) + 1.0))[0][0])\n",
    "\n",
    "    am_sum_2 = int(np.sum(am_2))\n",
    "    n_r_pred_2 = n_r_pred_2[:am_sum_2]\n",
    "    n_max_r_pred_2 = np.max(n_r_pred_2)\n",
    "    if n_max_r_pred_2 > n_max_reward_2:\n",
    "        n_max_reward_2 = n_max_r_pred_2\n",
    "\n",
    "    n_min_r_pred_2 = np.min(n_r_pred_2)\n",
    "    if n_min_r_pred_2 < n_min_reward_2:\n",
    "        n_min_reward_2 = n_min_r_pred_2\n",
    "\n",
    "    n_ts_2 = np.arange(am_sum_2)\n",
    "    n_rewards_2.append(np.column_stack([n_ts_2, n_r_pred_2]))\n",
    "    all_rewards_2.append(n_r_pred_2)\n",
    "    n_weights_2 = n_weights_2[-1].reshape(1, 100, 100)\n",
    "    n_weights_2 = np.mean(n_weights_2, axis=1).reshape(\n",
    "        100,\n",
    "    )\n",
    "    n_weights_2 = n_weights_2[:am_sum_2]\n",
    "    n_all_weights_2.append(np.column_stack([n_ts_2, n_weights_2]))\n",
    "    n_max_w_2 = np.max(n_weights_2)\n",
    "    if n_max_w_2 > n_max_weight_2:\n",
    "        n_max_weight_2 = n_max_w_2\n",
    "\n",
    "    am_sum = int(np.sum(am))\n",
    "    n_r_pred = n_r_pred[:am_sum]\n",
    "    n_max_r_pred = np.max(n_r_pred)\n",
    "    if n_max_r_pred > n_max_reward:\n",
    "        n_max_reward = n_max_r_pred\n",
    "\n",
    "    n_min_r_pred = np.min(n_r_pred)\n",
    "    if n_min_r_pred < n_min_reward:\n",
    "        n_min_reward = n_min_r_pred\n",
    "\n",
    "    n_ts = np.arange(am_sum)\n",
    "    n_rewards.append(np.column_stack([n_ts, n_r_pred]))\n",
    "    all_rewards.append(n_r_pred)\n",
    "    n_weights = n_weights[-1].reshape(1, 100, 100)\n",
    "    n_weights = np.mean(n_weights, axis=1).reshape(\n",
    "        100,\n",
    "    )\n",
    "    n_weights = n_weights[:am_sum]\n",
    "    n_all_weights.append(np.column_stack([n_ts, n_weights]))\n",
    "    n_max_w = np.max(n_weights)\n",
    "    if n_max_w > n_max_weight:\n",
    "        n_max_weight = n_max_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9b8e6-875d-4203-99ac-0eeb94567327",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_control_idxs_2 = []\n",
    "end_control_idxs_2 = []\n",
    "for i in range(100):\n",
    "    if acts_2[0, i, 2] == 1:\n",
    "        if i == 0:\n",
    "            start_control_idxs_2.append(i)\n",
    "        else:\n",
    "            if acts_2[0, i - 1, 2] == 0:\n",
    "                start_control_idxs_2.append(i)\n",
    "    else:\n",
    "        if acts_2[0, i - 1, 2] == 1:\n",
    "            end_control_idxs_2.append(i)\n",
    "\n",
    "start_control_idxs = []\n",
    "end_control_idxs = []\n",
    "for i in range(100):\n",
    "    if acts[0, i, 2] == 1:\n",
    "        if i == 0:\n",
    "            start_control_idxs.append(i)\n",
    "        else:\n",
    "            if acts[0, i - 1, 2] == 0:\n",
    "                start_control_idxs.append(i)\n",
    "    else:\n",
    "        if acts[0, i - 1, 2] == 1:\n",
    "            end_control_idxs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b697e1a-7708-4d9c-ba7b-c271102c2af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rewards_2 = np.stack(all_rewards_2)\n",
    "norm_rewards_2 = (all_rewards_2 - np.min(all_rewards_2, axis=0)) / (\n",
    "    np.max(all_rewards_2, axis=0) - np.min(all_rewards_2, axis=0)\n",
    ")\n",
    "\n",
    "all_rewards = np.stack(all_rewards)\n",
    "norm_rewards = (all_rewards - np.min(all_rewards, axis=0)) / (\n",
    "    np.max(all_rewards, axis=0) - np.min(all_rewards, axis=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862bae41-812b-4bfa-be1b-5c1be508ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axe = plt.subplots(3, 2, figsize=(15, 10))\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "axe[0, 0].set_xlim(0, am_sum_2)\n",
    "axe[0, 0].set_ylim(n_min_reward_2, n_max_reward_2)\n",
    "n_days_2 = np.arange(len(n_all_weights_2))\n",
    "n_line_collection0_2 = LineCollection(n_rewards_2, array=n_days_2, cmap=\"rainbow\")\n",
    "axe[0, 0].add_collection(n_line_collection0_2)\n",
    "axe[0, 0].vlines(\n",
    "    start_control_idxs_2,\n",
    "    n_min_reward_2,\n",
    "    n_max_reward_2,\n",
    "    color=\"green\",\n",
    "    linestyle=\"dashed\",\n",
    "    label=\"Start Human Control\",\n",
    ")\n",
    "axe[0, 0].vlines(\n",
    "    end_control_idxs_2,\n",
    "    n_min_reward_2,\n",
    "    n_max_reward_2,\n",
    "    color=\"magenta\",\n",
    "    linestyle=\"dashed\",\n",
    "    label=\"End Human Control\",\n",
    ")\n",
    "# axe[0,0].errorbar(np.arange(am_sum_2),np.mean(all_rewards_2,axis=0),yerr=np.std(all_rewards_2,axis=0),color=\"black\",label=\"Mean\")\n",
    "# axe[0,0].plot(np.arange(am_sum_2),np.median(all_rewards_2,axis=0),color=\"green\",label=\"Median\")\n",
    "axe[0, 0].set_title(\"Reward Signal (t0012)\")\n",
    "axe[0, 0].set_xlabel(\"Timestep\")\n",
    "axe[0, 0].set_ylabel(\"Reward\")\n",
    "axe[0, 0].legend(loc=\"upper left\")\n",
    "fig.colorbar(n_line_collection0_2, label=\"Day\")\n",
    "\n",
    "axe[1, 0].set_xlim(0, am_sum_2)\n",
    "axe[1, 0].set_ylim(0, n_max_weight_2)\n",
    "n_line_collection1_2 = LineCollection(n_all_weights_2, array=n_days_2, cmap=\"rainbow\")\n",
    "axe[1, 0].add_collection(n_line_collection1_2)\n",
    "axe[1, 0].vlines(\n",
    "    start_control_idxs_2,\n",
    "    0,\n",
    "    n_max_weight_2,\n",
    "    color=\"green\",\n",
    "    linestyle=\"dashed\",\n",
    "    label=\"Start Human Control\",\n",
    ")\n",
    "axe[1, 0].vlines(\n",
    "    end_control_idxs_2,\n",
    "    0,\n",
    "    n_max_weight_2,\n",
    "    color=\"magenta\",\n",
    "    linestyle=\"dashed\",\n",
    "    label=\"End Human Control\",\n",
    ")\n",
    "axe[1, 0].set_title(\"Importance Weighting (t0012)\")\n",
    "axe[1, 0].set_xlabel(\"Timestep\")\n",
    "axe[1, 0].set_ylabel(\"Weight\")\n",
    "axe[1, 0].legend(loc=\"upper left\")\n",
    "fig.colorbar(n_line_collection1_2, label=\"Day\")\n",
    "\n",
    "sns.heatmap(norm_rewards_2, cmap=\"cool\", ax=axe[2, 0], cbar_kws={\"label\": \"Reward\"})\n",
    "axe[2, 0].invert_yaxis()\n",
    "axe[2, 0].set_xlabel(\"Timestep\")\n",
    "axe[2, 0].set_ylabel(\"Day\")\n",
    "axe[2, 0].set_title(\"Relative Reward Signal Heatmap (t0012)\")\n",
    "\n",
    "axe[0, 1].set_xlim(0, am_sum)\n",
    "axe[0, 1].set_ylim(n_min_reward, n_max_reward)\n",
    "n_days = np.arange(len(n_all_weights))\n",
    "n_line_collection0 = LineCollection(n_rewards, array=n_days, cmap=\"rainbow\")\n",
    "axe[0, 1].add_collection(n_line_collection0)\n",
    "axe[0, 1].vlines(\n",
    "    start_control_idxs,\n",
    "    n_min_reward,\n",
    "    n_max_reward,\n",
    "    color=\"green\",\n",
    "    linestyle=\"dashed\",\n",
    "    label=\"Start Human Control\",\n",
    ")\n",
    "axe[0, 1].vlines(\n",
    "    end_control_idxs,\n",
    "    n_min_reward,\n",
    "    n_max_reward,\n",
    "    color=\"magenta\",\n",
    "    linestyle=\"dashed\",\n",
    "    label=\"End Human Control\",\n",
    ")\n",
    "# axe[0,1].errorbar(np.arange(am_sum),np.mean(all_rewards,axis=0),yerr=np.std(all_rewards,axis=0),color=\"black\",label=\"Mean\")\n",
    "# axe[0,1].plot(np.arange(am_sum),np.median(all_rewards,axis=0),color=\"green\",label=\"Median\")\n",
    "axe[0, 1].set_title(\"Reward Signal (t0012)\")\n",
    "axe[0, 1].set_xlabel(\"Timestep\")\n",
    "axe[0, 1].set_ylabel(\"Reward\")\n",
    "axe[0, 1].legend(loc=\"upper left\")\n",
    "fig.colorbar(n_line_collection0, label=\"Day\")\n",
    "\n",
    "axe[1, 1].set_xlim(0, am_sum)\n",
    "axe[1, 1].set_ylim(0, n_max_weight)\n",
    "n_line_collection1 = LineCollection(n_all_weights, array=n_days, cmap=\"rainbow\")\n",
    "axe[1, 1].add_collection(n_line_collection1)\n",
    "axe[1, 1].vlines(\n",
    "    start_control_idxs,\n",
    "    0,\n",
    "    n_max_weight,\n",
    "    color=\"green\",\n",
    "    linestyle=\"dashed\",\n",
    "    label=\"Start Human Control\",\n",
    ")\n",
    "axe[1, 1].vlines(\n",
    "    end_control_idxs,\n",
    "    0,\n",
    "    n_max_weight,\n",
    "    color=\"magenta\",\n",
    "    linestyle=\"dashed\",\n",
    "    label=\"End Human Control\",\n",
    ")\n",
    "axe[1, 1].set_title(\"Importance Weighting (t0012)\")\n",
    "axe[1, 1].set_xlabel(\"Timestep\")\n",
    "axe[1, 1].set_ylabel(\"Weight\")\n",
    "axe[1, 1].legend(loc=\"upper left\")\n",
    "fig.colorbar(n_line_collection1, label=\"Day\")\n",
    "\n",
    "sns.heatmap(norm_rewards, cmap=\"cool\", ax=axe[2, 1], cbar_kws={\"label\": \"Reward\"})\n",
    "axe[2, 1].invert_yaxis()\n",
    "axe[2, 1].set_xlabel(\"Timestep\")\n",
    "axe[2, 1].set_ylabel(\"Day\")\n",
    "axe[2, 1].set_title(\"Relative Reward Signal Heatmap (t0012)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76570a7b-5cd7-40e3-bf78-9812dc236b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(pr)\n",
    "ax.set_title(\"Preference Prediction (for segment 2) Versus Day\")\n",
    "ax.set_xlabel(\"Day\")\n",
    "ax.set_ylabel(\"Probability of Preferred Label\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
